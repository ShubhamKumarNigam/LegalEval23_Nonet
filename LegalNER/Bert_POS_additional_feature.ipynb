{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-25 14:03:13.901553: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-01-25 14:03:14.028419: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-10.0/lib64\n",
      "2023-01-25 14:03:14.028445: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-01-25 14:03:14.694518: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-10.0/lib64\n",
      "2023-01-25 14:03:14.694612: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-10.0/lib64\n",
      "2023-01-25 14:03:14.694621: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "from transformers.models.bert.modeling_bert import BertEncoder, BertPooler, BertEmbeddings, BaseModelOutputWithPoolingAndCrossAttentions\n",
    "from transformers import BertConfig, BertModel\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from ast import literal_eval\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from keras_preprocessing.sequence import pad_sequences    \n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# from tqdm import tqdm, trange\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "import pickle\n",
    "import torch\n",
    "from torchcrf import CRF\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import transformers\n",
    "from transformers import BertForTokenClassification, AdamW, BertModel, BertConfig\n",
    "from transformers import BertTokenizer, BertConfig\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import get_linear_schedule_with_warmup, get_constant_schedule_with_warmup, get_cosine_with_hard_restarts_schedule_with_warmup\n",
    "\n",
    "from tqdm import tqdm, trange\n",
    "import numpy as np \n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "import numpy as np\n",
    "# from seqeval.metrics import classification_report as classification_report_seqeval\n",
    "# from seqeval.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_gpu = torch.cuda.device_count()\n",
    "print(device)\n",
    "\n",
    "MAX_LEN = 128\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "import torch.nn.functional as F\n",
    "log_soft = F.log_softmax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertEmbeddingsV2(BertEmbeddings):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=config.pad_token_id)\n",
    "        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)\n",
    "        self.token_type_embeddings = nn.Embedding(config.type_vocab_size, config.hidden_size)\n",
    "        max_number_of_pos_tags = 34\n",
    "        self.pos_tag_embeddings = nn.Embedding(config.vocab_size, config.hidden_size)\n",
    "        \n",
    "        # self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load\n",
    "        # any TensorFlow checkpoint file\n",
    "        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "        # position_ids (1, len position emb) is contiguous in memory and exported when serialized\n",
    "        self.register_buffer(\"position_ids\", torch.arange(config.max_position_embeddings).expand((1, -1)))\n",
    "        self.position_embedding_type = getattr(config, \"position_embedding_type\", \"absolute\")\n",
    "\n",
    "    def forward(self, input_ids=None, pos_tag_ids=None, token_type_ids=None, position_ids=None, inputs_embeds=None, past_key_values_length=0):\n",
    "        if input_ids is not None:\n",
    "            input_shape = input_ids.size()\n",
    "        else:\n",
    "            input_shape = inputs_embeds.size()[:-1]\n",
    "\n",
    "        seq_length = input_shape[1]\n",
    "        # print(\"=======>\", input_shape)\n",
    "\n",
    "        if position_ids is None:\n",
    "            position_ids = self.position_ids[:, past_key_values_length : seq_length + past_key_values_length]\n",
    "\n",
    "        if token_type_ids is None:\n",
    "            token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=self.position_ids.device)\n",
    "\n",
    "        if inputs_embeds is None:\n",
    "            inputs_embeds = self.word_embeddings(input_ids)\n",
    "        token_type_embeddings = self.token_type_embeddings(token_type_ids)\n",
    "        pos_tag_embeddings = self.pos_tag_embeddings(pos_tag_ids)\n",
    "        \n",
    "        # print(\"INPUT_ID_SHAPE => \" ,inputs_embeds.shape)\n",
    "        # print(\"token_type_tag_ids1_SHAPE => \" ,token_type_embeddings.shape)\n",
    "        # print(\"pos_tag_ids1_SHAPE => \" ,pos_tag_embeddings.shape)\n",
    "\n",
    "        embeddings = inputs_embeds + token_type_embeddings + pos_tag_embeddings\n",
    "        if self.position_embedding_type == \"absolute\":\n",
    "            position_embeddings = self.position_embeddings(position_ids)\n",
    "            embeddings += position_embeddings\n",
    "        embeddings = self.LayerNorm(embeddings)\n",
    "        embeddings = self.dropout(embeddings)\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertModelV2(BertModel):\n",
    "    \"\"\"\n",
    "    The model can behave as an encoder (with only self-attention) as well as a decoder, in which case a layer of\n",
    "    cross-attention is added between the self-attention layers, following the architecture described in `Attention is\n",
    "    all you need <https://arxiv.org/abs/1706.03762>`__ by Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,\n",
    "    Llion Jones, Aidan N. Gomez, Lukasz Kaiser and Illia Polosukhin.\n",
    "    To behave as an decoder the model needs to be initialized with the :obj:`is_decoder` argument of the configuration\n",
    "    set to :obj:`True`. To be used in a Seq2Seq model, the model needs to initialized with both :obj:`is_decoder`\n",
    "    argument and :obj:`add_cross_attention` set to :obj:`True`; an :obj:`encoder_hidden_states` is then expected as an\n",
    "    input to the forward pass.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config, add_pooling_layer=True):\n",
    "        super().__init__(config)\n",
    "        self.config = config\n",
    "\n",
    "        self.embeddings = BertEmbeddingsV2(config)\n",
    "        self.encoder = BertEncoder(config)\n",
    "\n",
    "        self.pooler = BertPooler(config) if add_pooling_layer else None\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    \n",
    "    def forward(\n",
    "        self,input_ids=None,attention_mask=None,token_type_ids=None,pos_tag_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,encoder_hidden_states=None,encoder_attention_mask=None,\n",
    "        past_key_values=None,use_cache=None,output_attentions=None,output_hidden_states=True,return_dict=None,\n",
    "    ):\n",
    "        r\"\"\"\n",
    "        encoder_hidden_states  (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length, hidden_size)`, `optional`):\n",
    "            Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if\n",
    "            the model is configured as a decoder.\n",
    "        encoder_attention_mask (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n",
    "            Mask to avoid performing attention on the padding token indices of the encoder input. This mask is used in\n",
    "            the cross-attention if the model is configured as a decoder. Mask values selected in ``[0, 1]``:\n",
    "            - 1 for tokens that are **not masked**,\n",
    "            - 0 for tokens that are **masked**.\n",
    "        past_key_values (:obj:`tuple(tuple(torch.FloatTensor))` of length :obj:`config.n_layers` with each tuple having 4 tensors of shape :obj:`(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`):\n",
    "            Contains precomputed key and value hidden states of the attention blocks. Can be used to speed up decoding.\n",
    "            If :obj:`past_key_values` are used, the user can optionally input only the last :obj:`decoder_input_ids`\n",
    "            (those that don't have their past key value states given to this model) of shape :obj:`(batch_size, 1)`\n",
    "            instead of all :obj:`decoder_input_ids` of shape :obj:`(batch_size, sequence_length)`.\n",
    "        use_cache (:obj:`bool`, `optional`):\n",
    "            If set to :obj:`True`, :obj:`past_key_values` key value states are returned and can be used to speed up\n",
    "            decoding (see :obj:`past_key_values`).\n",
    "        \"\"\"\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "        )\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        if self.config.is_decoder:\n",
    "            use_cache = use_cache if use_cache is not None else self.config.use_cache\n",
    "        else:\n",
    "            use_cache = False\n",
    "\n",
    "        if input_ids is not None and inputs_embeds is not None:\n",
    "            raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n",
    "        elif input_ids is not None:\n",
    "            input_shape = input_ids.size()\n",
    "            batch_size, seq_length = input_shape\n",
    "        elif inputs_embeds is not None:\n",
    "            input_shape = inputs_embeds.size()[:-1]\n",
    "            batch_size, seq_length = input_shape\n",
    "        else:\n",
    "            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n",
    "\n",
    "        device = input_ids.device if input_ids is not None else inputs_embeds.device\n",
    "\n",
    "        # past_key_values_length\n",
    "        past_key_values_length = past_key_values[0][0].shape[2] if past_key_values is not None else 0\n",
    "\n",
    "        if attention_mask is None:\n",
    "            attention_mask = torch.ones(((batch_size, seq_length + past_key_values_length)), device=device)\n",
    "        if token_type_ids is None:\n",
    "            token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=device)\n",
    "\n",
    "        # We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]\n",
    "        # ourselves in which case we just need to make it broadcastable to all heads.\n",
    "        extended_attention_mask: torch.Tensor = self.get_extended_attention_mask(attention_mask, input_shape, device)\n",
    "\n",
    "        # If a 2D or 3D attention mask is provided for the cross-attention\n",
    "        # we need to make broadcastable to [batch_size, num_heads, seq_length, seq_length]\n",
    "        if self.config.is_decoder and encoder_hidden_states is not None:\n",
    "            encoder_batch_size, encoder_sequence_length, _ = encoder_hidden_states.size()\n",
    "            encoder_hidden_shape = (encoder_batch_size, encoder_sequence_length)\n",
    "            if encoder_attention_mask is None:\n",
    "                encoder_attention_mask = torch.ones(encoder_hidden_shape, device=device)\n",
    "            encoder_extended_attention_mask = self.invert_attention_mask(encoder_attention_mask)\n",
    "        else:\n",
    "            encoder_extended_attention_mask = None\n",
    "\n",
    "        # Prepare head mask if needed\n",
    "        # 1.0 in head_mask indicate we keep the head\n",
    "        # attention_probs has shape bsz x n_heads x N x N\n",
    "        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n",
    "        # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n",
    "        head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n",
    "\n",
    "        embedding_output = self.embeddings(\n",
    "            input_ids=input_ids,\n",
    "            position_ids=position_ids,\n",
    "            token_type_ids=token_type_ids,\n",
    "            pos_tag_ids=pos_tag_ids,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            past_key_values_length=past_key_values_length,\n",
    "        )\n",
    "        encoder_outputs = self.encoder(\n",
    "            embedding_output,\n",
    "            attention_mask=extended_attention_mask,\n",
    "            head_mask=head_mask,\n",
    "            encoder_hidden_states=encoder_hidden_states,\n",
    "            encoder_attention_mask=encoder_extended_attention_mask,\n",
    "            past_key_values=past_key_values,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        sequence_output = encoder_outputs[0]\n",
    "        pooled_output = self.pooler(sequence_output) if self.pooler is not None else None\n",
    "\n",
    "        if not return_dict:\n",
    "            return (sequence_output, pooled_output) + encoder_outputs[1:]\n",
    "\n",
    "        extend_out= BaseModelOutputWithPoolingAndCrossAttentions(\n",
    "            last_hidden_state=sequence_output,\n",
    "            pooler_output=pooled_output,\n",
    "            past_key_values=encoder_outputs.past_key_values,\n",
    "            hidden_states=encoder_outputs.hidden_states,\n",
    "            attentions=encoder_outputs.attentions,\n",
    "            cross_attentions=encoder_outputs.cross_attentions,\n",
    "        )\n",
    "        return encoder_outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"nlpaueb/legal-bert-base-uncased\")\n",
    "print(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dataset(file):\n",
    "    data = pd.read_csv(file)\n",
    "    return data\n",
    "\n",
    "def split_data(data):\n",
    "    X_label, POS, Y_label=[],[],[]\n",
    "\n",
    "    for i in range(len(data)):\n",
    "        words= list(data.sentence[i].split())\n",
    "        pos_tags = literal_eval(data.noun_features[i])\n",
    "        ner_tag = literal_eval(data.ner_features[i])\n",
    "        X_label.append(words)\n",
    "        POS.append(pos_tags)\n",
    "        Y_label.append(ner_tag)\n",
    "    return X_label, POS, Y_label\n",
    "    \n",
    "def create_dict(POS, y):\n",
    "    pos_set = set()\n",
    "    for li in POS:\n",
    "        for val in li:\n",
    "            pos_set.add(val)\n",
    "    ner_set = set()\n",
    "    for li in y:\n",
    "        for val in li:\n",
    "            ner_set.add(val)\n",
    "    ner_set=sorted(list(ner_set))\n",
    "    pos_set=sorted(list(pos_set))\n",
    "    train_tag_values = sorted(list(ner_set))\n",
    "    train_tag_values.append(\"PAD\")\n",
    "    pos2id={'NA': 0,'PAD': 1}\n",
    "    ner2id = {t: i for i, t in enumerate(train_tag_values)}\n",
    "\n",
    "    ind=2\n",
    "    for x in pos_set:\n",
    "        pos2id[x] = ind\n",
    "        ind+=1\n",
    "    return pos2id,ner2id,train_tag_values\n",
    "\n",
    "def _add_subword(tokenizer, words, pos_tags, ner_tag):\n",
    "    tokens = []\n",
    "    pos_tag_tokens = []\n",
    "    ner_tag_tokens =[]\n",
    "    for word, tag, ner in zip(words, pos_tags,ner_tag ):\n",
    "    # tokenize the word\n",
    "        word_tokens = tokenizer.tokenize(word)\n",
    "        tokens.extend(word_tokens)\n",
    "    # copy the POS tag and NER tag for all word tokens2\n",
    "        pos_tag_tokens.extend([tag for _ in range(len(word_tokens))])\n",
    "        ner_tag_tokens.extend([ner for _ in range(len(word_tokens))])\n",
    "    # tokens = [tokenizer.cls_token] + tokens + [tokenizer.sep_token]\n",
    "    # pos_tag_tokens = ['NA'] + pos_tag_tokens + ['NA']\n",
    "    # ner_tag_tokens = ['NA'] + ner_tag_tokens + ['NA']\n",
    "    return tokens,pos_tag_tokens,ner_tag_tokens\n",
    "    \n",
    "\n",
    "def add_subword2data(tokenizer,X,pos, Y):\n",
    "    '''\n",
    "            input:\n",
    "                sentence = [['Ayush', 'Kumar', 'Mishra',..],....]\n",
    "                text_labels = [['B-PER', 'I-PER','I-PER',..],...]\n",
    "\n",
    "            output: \n",
    "                [['Ayush', 'Kumar', 'Mish', '##ra',..],....],\n",
    "                [['B-PER', 'I-PER','I-PER','I-PER',..],...]\n",
    "    '''\n",
    "    tokenized_texts_and_labels = [_add_subword(tokenizer,sent,pos, labs) for sent,pos,labs in zip(X,pos, Y)]\n",
    "    text = [token_label_pair[0] for token_label_pair in tokenized_texts_and_labels]\n",
    "    pos = [token_label_pair[1] for token_label_pair in tokenized_texts_and_labels]\n",
    "    ner = [token_label_pair[2] for token_label_pair in tokenized_texts_and_labels]\n",
    "    return text,pos,ner\n",
    "\n",
    "def padding_data(tokenizer,X_subword,pos_subword, ner_subword,pos2idx,ner2idx,MAXLEN):\n",
    "    X_padding = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in X_subword],maxlen=MAXLEN, dtype=\"long\", value=0.0,truncating=\"post\", padding=\"post\")\n",
    "    pos_padding = pad_sequences([[pos2idx.get(l) for l in lab] for lab in pos_subword],maxlen=MAXLEN, value=pos2idx[\"PAD\"], padding=\"post\",dtype=\"long\", truncating=\"post\")\n",
    "    ner_padding = pad_sequences([[ner2idx.get(l) for l in lab] for lab in ner_subword],maxlen=MAXLEN, value=ner2idx[\"PAD\"], padding=\"post\",dtype=\"long\", truncating=\"post\")\n",
    "    attention_masks = [[int(i != 0) for i in ii] for ii in X_padding]\n",
    "    return X_padding, pos_padding, ner_padding, attention_masks\n",
    "\n",
    "def covert2tensor(input_ids,pos_tag_ids,ner_tag_ids,attention_mask,  mode ,device):\n",
    "    if mode == 'training':\n",
    "        input_ids = torch.tensor(input_ids).to(device)\n",
    "        pos_tag_ids = torch.tensor(pos_tag_ids).to(device) \n",
    "        ner_tag_ids = torch.tensor(ner_tag_ids).to(device) \n",
    "        attention_mask = torch.tensor(attention_mask).to(device)\n",
    "        \n",
    "\n",
    "    elif mode =='evaluation':\n",
    "        input_ids = torch.tensor(input_ids).type(torch.LongTensor).to(device)\n",
    "        pos_tag_ids = torch.tensor(pos_tag_ids).type(torch.LongTensor).to(device) \n",
    "        ner_tag_ids = torch.tensor(ner_tag_ids).type(torch.LongTensor).to(device) \n",
    "        attention_mask = torch.tensor(attention_mask).type(torch.LongTensor).to(device) \n",
    "        \n",
    "    return  input_ids,pos_tag_ids,ner_tag_ids,attention_mask\n",
    "\n",
    "def Dataloader(input_ids,pos_tag_ids,ner_tag_ids,attention_mask,BATCH_SIZE, mode ,type, device):\n",
    "    input_ids,pos_tag_ids,ner_tag_ids,attention_mask = covert2tensor(input_ids,pos_tag_ids,ner_tag_ids,attention_mask,  mode ,device)\n",
    "    if type == 'train':\n",
    "        train_data = TensorDataset(input_ids,pos_tag_ids,attention_mask,ner_tag_ids)\n",
    "        train_sampler = RandomSampler(train_data)\n",
    "        train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=BATCH_SIZE)\n",
    "        return train_dataloader\n",
    "    elif type == 'dev' or type == 'test':\n",
    "        valid_data = TensorDataset(input_ids,pos_tag_ids,attention_mask,ner_tag_ids)\n",
    "        valid_sampler = SequentialSampler(valid_data)\n",
    "        valid_dataloader = DataLoader(valid_data, sampler=valid_sampler, batch_size=BATCH_SIZE)\n",
    "        return valid_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_loader(dir,tokenizer,BATCH_SIZE,MAXLEN,mode,type_,device):\n",
    "    data = read_dataset(dir)\n",
    "    X,POS,y = split_data(data)\n",
    "    pos2id,ner2id,train_tag_values = create_dict(POS,y)\n",
    "    text,pos,ner= add_subword2data(tokenizer,X,POS,y)\n",
    "    X_padding, pos_subword, ner_subword, attention_masks = padding_data(tokenizer, text,pos,ner,pos2id,ner2id,MAXLEN)\n",
    "    loader = Dataloader(X_padding, pos_subword, ner_subword, attention_masks,BATCH_SIZE, mode ,type_ ,device)\n",
    "    return X,loader, pos2id, ner2id, train_tag_values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_ = 'train'\n",
    "BATCH_SIZE=32\n",
    "MAX_LEN = 128\n",
    "mode = 'training'\n",
    "dir_train=''\n",
    "train_set, train_loader, pos2id, ner2id, train_tag_values  = create_loader(dir_train, tokenizer, BATCH_SIZE, MAX_LEN, mode, type_, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_ = 'dev'\n",
    "BATCH_SIZE=32\n",
    "MAX_LEN = 128\n",
    "mode = 'evaluation'\n",
    "dir_val=''\n",
    "val_set, dev_loader, pos2id, ner2id, dev_labels  = create_loader(dir_val, tokenizer, BATCH_SIZE, MAX_LEN, mode, type_, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_ = 'test'\n",
    "BATCH_SIZE=32\n",
    "MAX_LEN = 128\n",
    "mode = 'evaluation'\n",
    "test_val=''\n",
    "test_set, test_loader, pos2id, ner2id, dev_labels  = create_loader(test_val, tokenizer, BATCH_SIZE, MAX_LEN, mode, type_, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = BertConfig.from_pretrained('nlpaueb/legal-bert-base-uncased', output_hidden_states=True)\n",
    "bert_model = BertModelV2.from_pretrained('nlpaueb/legal-bert-base-uncased',config=config,add_pooling_layer=False)\n",
    "print(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT_CRF(nn.Module):\n",
    "    def __init__(self, bert_model, num_labels):\n",
    "        super(BERT_CRF, self).__init__()\n",
    "        self.bert = bert_model\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "        # 4 last of layer\n",
    "        self.classifier = nn.Linear(4*768, num_labels)\n",
    "        self.crf = CRF(num_labels, batch_first = True)\n",
    "    \n",
    "    def forward_custom(self, input_ids,b_input_mask,pos_tag_ids,b_labels):\n",
    "        # outputs = self.bert(b_input_ids, attention_mask=b_input_mask)\n",
    "        outputs = self.bert.forward(input_ids=input_ids,attention_mask=b_input_mask,pos_tag_ids=pos_tag_ids)\n",
    "        sequence_output = torch.cat((outputs[1][-1], outputs[1][-2], outputs[1][-3], outputs[1][-4]),-1)\n",
    "        sequence_output = self.dropout(sequence_output)\n",
    "        \n",
    "        emission = self.classifier(sequence_output) # [32,256,17]\n",
    "        \n",
    "        \n",
    "        if b_labels is not None:\n",
    "            loss = -self.crf(log_soft(emission, 2), b_labels, mask=b_input_mask.type(torch.uint8), reduction='mean')\n",
    "            prediction = self.crf.decode(emission, mask=b_input_mask.type(torch.uint8))\n",
    "            return [loss, prediction]\n",
    "                \n",
    "        else:\n",
    "            prediction = self.crf.decode(emission, mask=b_input_mask.type(torch.uint8))\n",
    "            return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class BERT_BILSTM_CRF(nn.Module):\n",
    "#     def __init__(self, bert_model, num_labels):\n",
    "#         super(BERT_BILSTM_CRF, self).__init__()\n",
    "#         self.bert = bert_model\n",
    "#         self.dropout = nn.Dropout(0.25)\n",
    "#         # 4 last of layer\n",
    "#         rnn_dim = 2*768\n",
    "#         out_dim = 4*768\n",
    "#         need_birnn = True\n",
    "#         if need_birnn:\n",
    "#             self.birnn = nn.LSTM(out_dim, rnn_dim, num_layers=1, bidirectional=True, batch_first=True)\n",
    "#             out_dim = rnn_dim*2\n",
    "#         self.classifier = nn.Linear(out_dim, num_labels)\n",
    "#         self.crf = CRF(num_labels, batch_first = True)\n",
    "    \n",
    "#     def forward_custom(self, input_ids,b_input_mask,pos_tag_ids,b_labels):\n",
    "#         outputs = self.bert.forward(input_ids=input_ids,attention_mask=b_input_mask,pos_tag_ids=pos_tag_ids)\n",
    "#         sequence_output = torch.cat((outputs[1][-1], outputs[1][-2], outputs[1][-3], outputs[1][-4]),-1)\n",
    "#         # sequence_output = outputs[0]\n",
    "#         need_birnn = True\n",
    "#         if need_birnn:\n",
    "#             sequence_output, _ = self.birnn(sequence_output)\n",
    "#         sequence_output = self.dropout(sequence_output)\n",
    "#         emission = self.classifier(sequence_output) # [32,256,17]\n",
    "        \n",
    "#         if b_labels is not None:\n",
    "#             loss = -self.crf(log_soft(emission, 2), b_labels, mask=b_input_mask.type(torch.uint8), reduction='mean')\n",
    "#             prediction = self.crf.decode(emission, mask=b_input_mask.type(torch.uint8))\n",
    "#             return [loss, prediction]\n",
    "                \n",
    "#         else:\n",
    "#             prediction = self.crf.decode(emission, mask=b_input_mask.type(torch.uint8))\n",
    "#             return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "model = BERT_CRF(bert_model, num_labels=29)\n",
    "# model = BERT_CRF(bert_model, num_labels=29,dropout_rate=0.25,n_class=0,lstm_hidden_size=256)\n",
    "model.to(device)\n",
    "print(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = -1\n",
    "num_layer = 197 #?????\n",
    "for param in model.named_parameters():\n",
    "    cnt += 1\n",
    "    if cnt>=num_layer:\n",
    "        param[1].requires_grad = True\n",
    "    else:\n",
    "        param[1].requires_grad = True\n",
    "    # print(cnt,param[0],'\\t',param[1].requires_grad)\n",
    "    \n",
    "\n",
    "FINETUNING = True\n",
    "if FINETUNING:\n",
    "    param_optimizer1 = list(model.named_parameters())[:num_layer]\n",
    "    param_optimizer2 = list(model.named_parameters())[num_layer:num_layer+2]\n",
    "    param_optimizer3 = list(model.named_parameters())[num_layer+2:]\n",
    "    no_decay = ['bias', 'gamma', 'beta']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in param_optimizer1 if not any(nd in n for nd in no_decay)],\n",
    "         'weight_decay_rate': 1e-5},\n",
    "        {'params': [p for n, p in param_optimizer1 if any(nd in n for nd in no_decay)],\n",
    "         'weight_decay_rate': 0.0},\n",
    "        \n",
    "        {'params': [p for n, p in param_optimizer2 if not any(nd in n for nd in no_decay)],\n",
    "         'weight_decay_rate': 1e-3,\n",
    "         'lr': 1e-3},\n",
    "        {'params': [p for n, p in param_optimizer2 if any(nd in n for nd in no_decay)],\n",
    "         'weight_decay_rate': 0.0,\n",
    "         'lr':1e-3},\n",
    "        \n",
    "        {'params': [p for n, p in param_optimizer3 if not any(nd in n for nd in no_decay)],\n",
    "         'weight_decay_rate': 1e-3,\n",
    "         'lr':4e-3},\n",
    "        {'params': [p for n, p in param_optimizer3 if any(nd in n for nd in no_decay)],\n",
    "         'weight_decay_rate': 0.0,\n",
    "         'lr':4e-3}\n",
    "    ]\n",
    "    \n",
    "optimizer = AdamW(\n",
    "    optimizer_grouped_parameters,\n",
    "    lr=3e-5,\n",
    "    eps=1e-8\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 50\n",
    "max_grad_norm = 1.0\n",
    "\n",
    "# Total number of training steps is number of batches * number of epochs.\n",
    "total_steps = len(train_loader) * epochs\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=int(total_steps/10),\n",
    "    num_training_steps=total_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_path = ''\n",
    "PATH=''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "## Store the average loss after each epoch so we can plot them.\n",
    "train_loss_values, valid_loss_values = [], []\n",
    "f1_max = 0\n",
    "f1_list = []\n",
    "hist = {}\n",
    "\n",
    "\n",
    "for epoch in trange(epochs, desc=\"Epoch\"):\n",
    "    # ========================================\n",
    "    #               Training\n",
    "    # ========================================\n",
    "    # Perform one full pass over the training set.\n",
    "\n",
    "    # Put the model into training mode.\n",
    "    model.train()\n",
    "    # Reset the total loss for this epoch.\n",
    "    total_loss = 0\n",
    "\n",
    "    # Training loop\n",
    "    for step, batch in enumerate(train_loader):\n",
    "        # add batch to gpu\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "\n",
    "        # input_ids1,pos_tag_ids1,ner_tag_ids1,attention_mask1 = batch\n",
    "        input_ids1,pos_tag_ids1,attention_mask1,ner_tag_ids1 = batch\n",
    "\n",
    "        # print(\"INPUT_ID_SHAPE => \" ,input_ids1.shape)\n",
    "        # print(\"pos_tag_ids1_SHAPE => \" ,pos_tag_ids1.shape)\n",
    "        # print(\"attention_mask1_SHAPE => \" ,attention_mask1.shape)\n",
    "        # print(\"ner_tag_ids1_SHAPE => \" ,ner_tag_ids1.shape)\n",
    "\n",
    "        # Always clear any previously calculated gradients before performing a backward pass.\n",
    "        model.zero_grad()\n",
    "        \n",
    "        \n",
    "        # forward pass\n",
    "        # This will return the loss (rather than the model output)\n",
    "        # because we have provided the `labels`.\n",
    "        # input_ids,b_input_mask,pos_tag_ids,b_labels=None\n",
    "        outputs = model.forward_custom(input_ids=input_ids1,b_input_mask=attention_mask1,pos_tag_ids=pos_tag_ids1,b_labels=ner_tag_ids1)\n",
    "        loss = outputs[0]\n",
    "        # Perform a backward pass to calculate the gradients.\n",
    "        loss.backward()\n",
    "        # track train loss\n",
    "        total_loss += loss.item()\n",
    "        # Clip the norm of the gradient\n",
    "        # This is to help prevent the \"exploding gradients\" problem.\n",
    "        torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=max_grad_norm)\n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "        # Update the learning rate.\n",
    "        scheduler.step()\n",
    "\n",
    "    # Calculate the average loss over the training data.\n",
    "    avg_train_loss = total_loss / len(train_loader)\n",
    "    print(\"\\nAverage train loss: {}\".format(avg_train_loss))\n",
    "\n",
    "    # Store the loss value for plotting the learning curve.\n",
    "    train_loss_values.append(avg_train_loss)\n",
    "\n",
    "\n",
    "    # ========================================\n",
    "    #               Validation\n",
    "    # ========================================\n",
    "    model.eval()\n",
    "    predictions_f1 , true_labels_f1 = [], []\n",
    "    eval_loss = 0\n",
    "    for batch in dev_loader:\n",
    "\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        # b_input_ids, b_input_mask, b_labels = batch\n",
    "        # b_input_ids,pos_tag_ids1,b_labels,b_input_mask = batch\n",
    "        b_input_ids,pos_tag_ids1,b_input_mask,b_labels = batch\n",
    "\n",
    "        # print(\"INPUT_ID_SHAPE => \" ,b_input_ids.shape)\n",
    "        # print(\"pos_tag_ids1_SHAPE => \" ,pos_tag_ids1.shape)\n",
    "        # print(\"attention_mask1_SHAPE => \" ,b_input_mask.shape)\n",
    "        # print(\"ner_tag_ids1_SHAPE => \" ,b_labels.shape)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model.forward_custom(input_ids=b_input_ids,b_input_mask=b_input_mask,pos_tag_ids=pos_tag_ids1,b_labels=b_labels)\n",
    "\n",
    "        eval_loss += outputs[0].mean().item()\n",
    "        predict_labels = outputs[1]\n",
    "        label_ids = b_labels.to('cpu').numpy().tolist()\n",
    "        predictions = []\n",
    "        for predict_label in predict_labels:\n",
    "            predictions.append(predict_label)\n",
    "\n",
    "        for b_input_id, preds, labels in zip(b_input_ids, predictions, label_ids):\n",
    "            tokens = tokenizer.convert_ids_to_tokens(b_input_id.to('cpu').numpy())\n",
    "\n",
    "            new_tokens, new_labels, new_preds = [], [], []\n",
    "            for token, label_idx, pred in zip(tokens, labels, preds):\n",
    "                if token.startswith(\"##\"):\n",
    "                    new_tokens[-1] = new_tokens[-1] + token[2:]\n",
    "                else:\n",
    "                    new_labels.append(label_idx)\n",
    "                    new_preds.append(pred)\n",
    "                    new_tokens.append(token)\n",
    "            for token, pred, label in zip(new_tokens, new_preds, new_labels):\n",
    "                predictions_f1.extend([pred])\n",
    "                true_labels_f1.extend([label])\n",
    "    eval_loss = eval_loss / len(dev_loader)\n",
    "    print(\"Validation loss:\", eval_loss)\n",
    "    true_labels_f1_convert = [train_tag_values[i] for i in true_labels_f1]\n",
    "    predictions_f1_convert = [train_tag_values[i] for i in predictions_f1]\n",
    "    f1 = f1_score(true_labels_f1_convert, predictions_f1_convert,labels= dev_labels ,average='macro')\n",
    "    print(classification_report(true_labels_f1_convert, predictions_f1_convert, labels= dev_labels, digits=4))\n",
    "    \n",
    "\n",
    "    # eval_loss, f1 = evaluate(BIO_dataloader_valid,device)\n",
    "    valid_loss_values.append(eval_loss)\n",
    "    f1_list.append(f1)\n",
    "    hist['train_loss_values'] = train_loss_values\n",
    "    hist['valid_loss_values'] = valid_loss_values\n",
    "    hist['f1_list'] = f1_list\n",
    "    if f1 > f1_max:\n",
    "      \n",
    "        print(f'f1_score improved from: {f1_max:.4f} to {f1:.4f}')\n",
    "        print(f'Best model saved to {PATH}')\n",
    "        f1_max = f1\n",
    "        # torch.save({\n",
    "        #     'epoch': epoch,\n",
    "        #     'f1_score': f1_max,\n",
    "        #     'history': hist,\n",
    "        #     'model_state_dict': model.state_dict(),\n",
    "        #     'optomizer_state_dict': optimizer.state_dict()\n",
    "        # }, PATH)\n",
    "        torch.save(model.state_dict(), model_save_path)\n",
    "                    # also save the optimizers' state\n",
    "        torch.save(optimizer.state_dict(), model_save_path + '.optim')\n",
    "        patience = 15\n",
    "        epochs_no_improve = 0\n",
    "        best_epoch = epoch\n",
    "    else:\n",
    "        print(f'f1_score dont improve from: {f1_max:.4f} to {f1:.4f}')\n",
    "        epochs_no_improve += 1\n",
    "        if epochs_no_improve < patience:\n",
    "            print(f'EarlyStopping count: {epochs_no_improve}/{patience}')\n",
    "        else:\n",
    "            print(f'\\nEarly Stopping! Total epochs: {epochs}. Best epoch: {best_epoch} with f1_score: {f1_max:.4f}')\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_f1 , true_labels_f1 = [], []\n",
    "word_len_list=[]\n",
    "word_list=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_ = 'test'\n",
    "BATCH_SIZE=32\n",
    "MAX_LEN = 128\n",
    "mode = 'evaluation'\n",
    "test_val=''\n",
    "test_set, test_loader, pos2id, ner2id, dev_labels  = create_loader(test_val, tokenizer, BATCH_SIZE, MAX_LEN, mode, type_, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions , true_labels = [], []\n",
    "sentencs=[]\n",
    "\n",
    "# Predict \n",
    "for batch in test_loader:\n",
    "  # Add batch to GPU\n",
    "  batch = tuple(t.to(device) for t in batch)\n",
    "  \n",
    "  # Unpack the inputs from our dataloader\n",
    "  # b_input_ids, b_input_mask, b_labels = batch\n",
    "\n",
    "  b_input_ids,pos_tag_ids1,b_input_mask,b_labels = batch\n",
    "\n",
    "  \n",
    "  # Telling the model not to compute or store gradients, saving memory and \n",
    "  # speeding up prediction\n",
    "  with torch.no_grad():\n",
    "    # model.forward_custom(b_input_ids, b_input_mask, b_labels, token_type_ids=None)\n",
    "      # Forward pass, calculate logit predictions\n",
    "      outputs = model.forward_custom(input_ids=b_input_ids,b_input_mask=b_input_mask,pos_tag_ids=pos_tag_ids1,b_labels=b_labels)\n",
    "  test_loss=0\n",
    "  test_loss += outputs[0].mean().item()\n",
    "  predict_labels = outputs[1]\n",
    "  label_ids = b_labels.to('cpu').numpy().tolist()\n",
    "  predictions = []\n",
    "  for predict_label in predict_labels:\n",
    "      predictions.append(predict_label)\n",
    "\n",
    "  for b_input_id, preds, labels in zip(b_input_ids, predictions, label_ids):\n",
    "      tokens = tokenizer.convert_ids_to_tokens(b_input_id.to('cpu').numpy())\n",
    "\n",
    "      new_tokens, new_labels, new_preds = [], [], []\n",
    "      for token, label_idx, pred in zip(tokens, labels, preds):\n",
    "          if token.startswith(\"##\"):\n",
    "              new_tokens[-1] = new_tokens[-1] + token[2:]\n",
    "          else:\n",
    "              new_labels.append(label_idx)\n",
    "              new_preds.append(pred)\n",
    "              new_tokens.append(token)\n",
    "      count=0\n",
    "      for token, pred, label in zip(new_tokens, new_preds, new_labels):\n",
    "          count+=1\n",
    "          predictions_f1.extend([pred])\n",
    "          true_labels_f1.extend([label])\n",
    "          sentencs.extend([token])\n",
    "          word_list.append(new_tokens)\n",
    "      word_len_list.append(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sentencs), len(predictions_f1), len(word_len_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_f1_convert = [train_tag_values[i] for i in predictions_f1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_sentence = [i for i in sentencs]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(word_sentence), len(predictions_f1_convert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_sentence[0],predictions_f1_convert[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_labels_f1_convert = [train_tag_values[i] for i in true_labels_f1]\n",
    "predictions_f1_convert = [train_tag_values[i] for i in predictions_f1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count=0\n",
    "for  i in range(len(true_labels_f1_convert)):\n",
    "    if true_labels_f1_convert[i] !=predictions_f1_convert[i]:\n",
    "        count+=1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(len(true_labels_f1_convert)-count)/len(true_labels_f1_convert)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
